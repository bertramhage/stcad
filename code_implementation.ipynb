{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d86d38",
   "metadata": {},
   "source": [
    "# Code Implementation\n",
    "This file contains the main pipeline for the project.\n",
    "\n",
    "Additional helper functions and modules can be found under `src/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1b9970",
   "metadata": {},
   "source": [
    "## Setup environment (if not done)\n",
    "1. **Create and activate a virtual environment**\\\n",
    "```bash\n",
    "python3.11 -m venv .venv\n",
    ". .venv/bin/activate\n",
    "```\n",
    "\n",
    "2. **Install requirements** \\\n",
    "This project uses uv. uv can be installed with `pip install uv`.\\\n",
    "Run to install requirements\n",
    "```bash\n",
    "uv sync\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4907e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared imports\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import joblib\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "data_dir = \"data/ais/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe38586",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "The data used in this project is maritime data from automatic identification systems (AIS) obtained obtained from the [Danish Maritime Authority](http://aisdata.ais.dk/). The data is available as a csv file for each day and contains a row for each AIS message with columns such as **Timestamp**, **MMSI**, **Latitude**, **Longitude**, and more. MMSI stands for Maritime Mobile Service Identity and is a unique identifier for a vessel.\n",
    "\n",
    "For this notebook we use a subset of the data by using data only for 3 days: `2024-05-01`, `2024-05-02`, `2024-05-03`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d93f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AIS data download from 2024-05-01 to 2024-05-03 into data/ais/\n",
      "Downloading: http://aisdata.ais.dk/2024/aisdk-2024-05-01.zip\n",
      "Downloading: http://aisdata.ais.dk/2024/aisdk-2024-05-02.zip\n",
      "Downloading: http://aisdata.ais.dk/2024/aisdk-2024-05-03.zip\n",
      "\n",
      "All files downloaded successfully.\n",
      "End of download process.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def download_ais_data(from_date: date, to_date: date, destination_path: str):\n",
    "    \"\"\"Downloads and unzips AIS data for a given date range.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(destination_path):\n",
    "        os.makedirs(destination_path)\n",
    "        \n",
    "    print(f\"Starting AIS data download from {from_date} to {to_date} into {destination_path}\")\n",
    "\n",
    "    base_url = \"http://aisdata.ais.dk/\"\n",
    "    current_date = from_date\n",
    "    \n",
    "    errors = []\n",
    "    successes = 0\n",
    "    while current_date <= to_date:\n",
    "        year = current_date.strftime(\"%Y\")\n",
    "        month = current_date.strftime(\"%m\")\n",
    "        day = current_date.strftime(\"%d\")\n",
    "        \n",
    "        file_name = f\"aisdk-{year}-{month}-{day}.zip\" # Construct the file name and URL\n",
    "        file_url = f\"{base_url}{year}/{file_name}\"\n",
    "        \n",
    "        print(f\"Downloading: {file_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with io.BytesIO(response.content) as zip_buffer:\n",
    "                    with zipfile.ZipFile(zip_buffer, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(destination_path)\n",
    "                        unzipped_files = zip_ref.namelist()\n",
    "                        assert len(unzipped_files) == 1, \"Expected exactly one file in the zip archive.\"\n",
    "                        successes += 1\n",
    "                        \n",
    "            elif response.status_code == 404:\n",
    "                print(f\"Data not found for {current_date} (404 Error). Skipping.\")\n",
    "                errors.append((current_date, \"404 Not Found\"))\n",
    "            else:\n",
    "                print(f\"Failed to download {file_name}. Status code: {response.status_code}\")\n",
    "                errors.append((current_date, f\"HTTP {response.status_code}\"))\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred during download for {current_date}: {e}\")\n",
    "            errors.append((current_date, str(e)))\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"Downloaded file for {current_date} is not a valid zip file.\")\n",
    "            errors.append((current_date, \"Bad Zip File\"))\n",
    "        except AssertionError as ae:\n",
    "            print(f\"Assertion error for {current_date}: {ae}\")\n",
    "            errors.append((current_date, str(ae)))\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    if len(errors) == 0:\n",
    "        print(\"\\nAll files downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"\\nDownload succeeded for {successes}/{(successes + len(errors))} days.\")\n",
    "        print(f\"Errors encountered for the following dates:\")\n",
    "        for err_date, err_msg in errors:\n",
    "            print(f\" - {err_date}: {err_msg}\")\n",
    "    print(\"End of download process.\")\n",
    "    \n",
    "download_ais_data(date(2024, 5, 1), date(2024, 5, 3), data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d6e81",
   "metadata": {},
   "source": [
    "## Data preprocessing using MapReduce\n",
    "Uncompressed, the data for a single day takes up around 3GB of memory and we wish to process 3 months worth of data leading to an infeasible amount of data to keep in memory at one time. However, since the data is time series data and vessel voyages often spans across days, in order to properly preprocess the data we can't process the files in isolation. Secondly, we wish to speed up the wall clock time of preprocessing by efficiently utilizing parallel processing on multiple CPU's running on DTU's High Performance Computing (HPC) cluster. This is where MapReduce comes in.\n",
    "\n",
    "### Split\n",
    "We first converts each CSV file individually to dictionaries of arrays grouped by MMSI. The grouped dictionaries are saved as pickle files in a temporary directory. This is the split part of MapReduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601663da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 CSV files in data/ais/.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993305b3a964413fb86b1c059d270da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading csvs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d27323b381b488495ade2c046d1c038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building track lists...:   0%|          | 0/15464605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eef36f9cd6e4a7796f3808d44dc74c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sorting and converting to NumPy...:   0%|          | 0/3963 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1de7c68c394063b6aa7908720e273b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building track lists...:   0%|          | 0/16040042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ce5f1a3cbb4e15aac0448c00faf7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sorting and converting to NumPy...:   0%|          | 0/3562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37a8d53e84446ad80e9ef01c0561257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building track lists...:   0%|          | 0/15343098 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2645c274ec91402d84a3c45b5b9d2c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sorting and converting to NumPy...:   0%|          | 0/3891 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed.\n",
      "Total messages processed: 60839630\n",
      "Total messages after filtering: 46847745\n",
      "Total unique vessels: 5022\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from collections import defaultdict\n",
    "from src.preprocessing.preprocessing import LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, SPEED_MAX as SOG_MAX\n",
    "from src.preprocessing.csv2pkl import SHIP_TYPE_MAP, NAV_STT_MAP\n",
    "\n",
    "# Define column indices\n",
    "LAT, LON, SOG, COG, HEADING, ROT, NAV_STT, TIMESTAMP, MMSI, SHIPTYPE  = list(range(10))\n",
    "\n",
    "def csv2pkl(input_dir=\"data/files/\",\n",
    "            output_dir=\"data/pickle_files\"):\n",
    "    \"\"\" Converts raw AIS CSV files to dictionaries grouped by MMSI and saves them as pickle files.\"\"\"\n",
    "    \n",
    "    global LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, SOG_MAX, SHIP_TYPE_MAP, NAV_STT_MAP\n",
    "      \n",
    "    l_csv_filename = [filename for filename in os.listdir(input_dir) if filename.endswith('.csv')]\n",
    "    print(f\"Found {len(l_csv_filename)} CSV files in {input_dir}.\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    results = {file_name: {\"total_messages\": 0, \"filtered_messages\": 0} for file_name in l_csv_filename}\n",
    "\n",
    "    messages_processed = 0\n",
    "    unique_vessels = set()\n",
    "    for csv_filename in tqdm(l_csv_filename, desc=f'Reading csvs'):\n",
    "        try:\n",
    "            t_date_str = '-'.join(csv_filename.split('.')[0].split('-')[1:4])\n",
    "            t_min = time.mktime(time.strptime(t_date_str + ' 00:00:00', \"%Y-%m-%d %H:%M:%S\"))\n",
    "            t_max = time.mktime(time.strptime(t_date_str + ' 23:59:59', \"%Y-%m-%d %H:%M:%S\"))\n",
    "            \n",
    "            # Lazy load data using Polars\n",
    "            lf = pl.scan_csv(os.path.join(input_dir, csv_filename),\n",
    "                            schema_overrides={\n",
    "                                \"# Timestamp\": pl.Utf8,\n",
    "                                \"MMSI\": pl.Int64,\n",
    "                                \"Latitude\": pl.Float64,\n",
    "                                \"Longitude\": pl.Float64,\n",
    "                                \"Navigational status\": pl.Utf8,\n",
    "                                \"ROT\": pl.Float64,\n",
    "                                \"SOG\": pl.Float64,\n",
    "                                \"COG\": pl.Float64,\n",
    "                                \"Heading\": pl.Int64,\n",
    "                                \"Ship type\": pl.Utf8\n",
    "                            })\n",
    "            total_messages = lf.select(pl.len()).collect()[0,0]\n",
    "            messages_processed += total_messages\n",
    "            results[csv_filename][\"total_messages\"] = total_messages\n",
    "\n",
    "            lf = (\n",
    "                lf.with_columns(\n",
    "                    pl.col(\"# Timestamp\").str.to_datetime(\"%d/%m/%Y %H:%M:%S\").dt.epoch(\"s\").alias(\"Timestamp\"), # Convert to UNIX timestamp\n",
    "                    pl.col(\"Navigational status\").replace_strict(NAV_STT_MAP, default=15) # Map navigational status to integers\n",
    "                ).filter(\n",
    "                    (pl.col(\"Latitude\") >= LAT_MIN) & (pl.col(\"Latitude\") <= LAT_MAX) &\n",
    "                    (pl.col(\"Longitude\") >= LON_MIN) & (pl.col(\"Longitude\") <= LON_MAX) &\n",
    "                    (pl.col(\"SOG\") >= 0) & (pl.col(\"SOG\") <= SOG_MAX) &\n",
    "                    (pl.col(\"COG\") >= 0) & (pl.col(\"COG\") <= 360) &\n",
    "                    (pl.col(\"Timestamp\") >= t_min) & (pl.col(\"Timestamp\") <= t_max)\n",
    "                ).select( # Select only the 9 columns needed for the track + ship type\n",
    "                    pl.col(\"Latitude\"),\n",
    "                    pl.col(\"Longitude\"),\n",
    "                    pl.col(\"SOG\"),\n",
    "                    pl.col(\"COG\"),\n",
    "                    pl.col(\"Heading\"),\n",
    "                    pl.col(\"ROT\"),\n",
    "                    pl.col(\"Navigational status\"),\n",
    "                    pl.col(\"Timestamp\"),\n",
    "                    pl.col(\"MMSI\"),\n",
    "                    pl.col(\"Ship type\")\n",
    "                )\n",
    "            )\n",
    "                    \n",
    "            ### Vessel Type Mapping\n",
    "            vessel_type_dir = os.path.join(output_dir, \"vessel_types\")\n",
    "            os.makedirs(vessel_type_dir, exist_ok=True)\n",
    "            vt_df = (\n",
    "                lf.with_columns(\n",
    "                    pl.col(\"Ship type\").replace_strict(SHIP_TYPE_MAP, default=0)\n",
    "                )\n",
    "                .filter(pl.col(\"Ship type\") != 0) # \"Undefined\"\n",
    "                .group_by(\"MMSI\")\n",
    "                .agg(\n",
    "                    pl.col(\"Ship type\").mode().first().alias(\"VesselType\")  # If multiple use the most frequent type\n",
    "                )\n",
    "                .collect()\n",
    "            )\n",
    "            \n",
    "            unique_vessels.update(vt_df[\"MMSI\"].to_list())\n",
    "            \n",
    "            # Save vessel types mapping\n",
    "            VesselTypes = {row[0]: row[1] for row in vt_df.iter_rows()}\n",
    "            vt_output_filename = csv_filename.replace('csv', 'pkl')\n",
    "            with open(os.path.join(vessel_type_dir, vt_output_filename), \"wb\") as f:\n",
    "                pickle.dump(VesselTypes, f)\n",
    "                \n",
    "            df = lf.drop(\"Ship type\").collect() # Ship type column no longer needed\n",
    "            results[csv_filename][\"filtered_messages\"] = df.height\n",
    "            \n",
    "            # Build tracks\n",
    "            Vs_list = defaultdict(list)\n",
    "            for row_tuple in tqdm(df.iter_rows(named=False), total=len(df), desc=\"Building track lists...\", leave=False):\n",
    "                mmsi = row_tuple[MMSI] \n",
    "                Vs_list[mmsi].append(row_tuple)\n",
    "            \n",
    "            Vs = {} # Final dictionary\n",
    "            for mmsi, track_list in tqdm(Vs_list.items(), desc=\"Sorting and converting to NumPy...\", leave=False):\n",
    "                track_list.sort(key=lambda x: x[TIMESTAMP])\n",
    "                Vs[mmsi] = np.array(track_list, dtype=np.float64)\n",
    "            \n",
    "            output_filename = csv_filename.replace('csv', 'pkl') \n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            joblib.dump(Vs, output_path, compress=3)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {csv_filename}: {e}\")\n",
    "        \n",
    "    print(\"Conversion completed.\")\n",
    "    \n",
    "    total_messages = sum(info[\"total_messages\"] for info in results.values())\n",
    "    total_filtered = sum(info[\"filtered_messages\"] for info in results.values())\n",
    "    print(f\"Total messages processed: {total_messages}\")\n",
    "    print(f\"Total messages after filtering: {total_filtered}\")\n",
    "    print(f\"Total unique vessels: {len(unique_vessels)}\")\n",
    "    \n",
    "csv2pkl(input_dir=data_dir, output_dir=f\"{data_dir}/pickle_files/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1728809",
   "metadata": {},
   "source": [
    "### Mapping and shuffling\n",
    "Now that the full dataset has been chunked (split) we map each item (trajectory) based on MMSI to a MMSI directory ready for preprocessing (reduction).\n",
    "\n",
    "The resulting temporary directory has the structure:\\\n",
    "```\n",
    "data/\n",
    "└── temp_dir/\n",
    "    ├── 123456789/                      # MMSI (unique vessel identifier)\n",
    "    │   ├── chunk_0001.pkl              # Segment(s) from input_dir\n",
    "    │   ├── chunk_0002.pkl\n",
    "    │\n",
    "    ├── 987654321/\n",
    "    │   ├── chunk_0001.pkl\n",
    "    │\n",
    "    └── ...                             # One folder per MMSI\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e10036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting map and shuffle phase on 3 files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d7cb9ecce4453bb44e172cb91d6423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map and Shuffle:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map and shuffle phase completed.\n"
     ]
    }
   ],
   "source": [
    "def map_and_shuffle(input_dir: str, temp_dir: str):\n",
    "    \"\"\" Goes through all input files and re-sorts them by MMSI into a temporary directory. \"\"\"\n",
    "    \n",
    "    input_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".pkl\")] # Input files from chunking step\n",
    "    \n",
    "    print(f\"Starting map and shuffle phase on {len(input_files)} files...\")\n",
    "\n",
    "    for file_path in tqdm(input_files, desc=\"Map and Shuffle\"):\n",
    "        data_dict = joblib.load(file_path)\n",
    "            \n",
    "        for mmsi, track_segment in data_dict.items():\n",
    "            \n",
    "            # Create a directory for this specific MMSI\n",
    "            mmsi_dir = os.path.join(temp_dir, str(mmsi))\n",
    "            os.makedirs(mmsi_dir, exist_ok=True)\n",
    "            \n",
    "            # Save this segment into the MMSI's folder\n",
    "            # We name it after the original file to avoid collisions\n",
    "            segment_filename = os.path.basename(file_path)\n",
    "            output_path = os.path.join(mmsi_dir, segment_filename)\n",
    "            \n",
    "            joblib.dump(track_segment, output_path, compress=3)\n",
    "    \n",
    "    print(\"Map and shuffle phase completed.\")\n",
    "\n",
    "map_and_shuffle(input_dir=f\"{data_dir}/pickle_files/\", temp_dir=f\"{data_dir}/temp_dir/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849553f7",
   "metadata": {},
   "source": [
    "### Reduce (in parallel)\n",
    "In the final step of the MapReduce algorithm, the reduction step, we apply preprocessing of the vessel trajectories. As we consider vessels' trajectories as independent from each other, and we have split and shuffled the trajectories by MMSI in the previous step, we are able to perform this step in parallel. \n",
    "\n",
    "The preprocessing includes identifying a vessels \"voyages\". We define a voyage as a contiguous sequence of AIS messages from the same vessel (possible across days), where the time interval between any two consecutive messages does not exceed two hours, and the vessel is actively moving (i.e., not moored or at anchor). See [D. Nguyen, R. Fablet](https://arxiv.org/pdf/2109.03958) for the full preprocessing rules implemented.\n",
    "\n",
    "The folder structure for the finally preprocessed files will look like:\n",
    "```\n",
    "processed/\n",
    "├── 123456789_0_processed.pkl            # Processed trajectory for MMSI 123456789 (segment 0)\n",
    "├── 123456789_1_processed.pkl            # (if multiple processed trajectories exist for same MMSI)\n",
    "├── 987654321_0_processed.pkl\n",
    "├── 987654321_1_processed.pkl\n",
    "└── ...\n",
    "```\n",
    "where each pickle file constitutes one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943375dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 7 workers for reduce phase.\n",
      "Starting reduce phase on 5481 MMSI folders\n",
      "Processed 100/5481 MMSIs. Avg wall time per MMSI: 0.1701s. Errors so far: 0.\n",
      "Processed 200/5481 MMSIs. Avg wall time per MMSI: 0.0486s. Errors so far: 0.\n",
      "Processed 300/5481 MMSIs. Avg wall time per MMSI: 0.0702s. Errors so far: 0.\n",
      "Processed 400/5481 MMSIs. Avg wall time per MMSI: 0.0258s. Errors so far: 0.\n",
      "Processed 500/5481 MMSIs. Avg wall time per MMSI: 0.0319s. Errors so far: 0.\n",
      "Processed 600/5481 MMSIs. Avg wall time per MMSI: 0.0206s. Errors so far: 0.\n",
      "Processed 700/5481 MMSIs. Avg wall time per MMSI: 0.0163s. Errors so far: 0.\n",
      "Processed 800/5481 MMSIs. Avg wall time per MMSI: 0.0189s. Errors so far: 0.\n",
      "Processed 900/5481 MMSIs. Avg wall time per MMSI: 0.0139s. Errors so far: 0.\n",
      "Processed 1000/5481 MMSIs. Avg wall time per MMSI: 0.0147s. Errors so far: 0.\n",
      "Processed 1100/5481 MMSIs. Avg wall time per MMSI: 0.0093s. Errors so far: 0.\n",
      "Processed 1200/5481 MMSIs. Avg wall time per MMSI: 0.0116s. Errors so far: 0.\n",
      "Processed 1300/5481 MMSIs. Avg wall time per MMSI: 0.0073s. Errors so far: 0.\n",
      "Processed 1400/5481 MMSIs. Avg wall time per MMSI: 0.0071s. Errors so far: 0.\n",
      "Processed 1500/5481 MMSIs. Avg wall time per MMSI: 0.0077s. Errors so far: 0.\n",
      "Processed 1600/5481 MMSIs. Avg wall time per MMSI: 0.0072s. Errors so far: 0.\n",
      "Processed 1700/5481 MMSIs. Avg wall time per MMSI: 0.0085s. Errors so far: 0.\n",
      "Processed 1800/5481 MMSIs. Avg wall time per MMSI: 0.0062s. Errors so far: 0.\n",
      "Processed 1900/5481 MMSIs. Avg wall time per MMSI: 0.0079s. Errors so far: 0.\n",
      "Processed 2000/5481 MMSIs. Avg wall time per MMSI: 0.0053s. Errors so far: 0.\n",
      "Processed 2100/5481 MMSIs. Avg wall time per MMSI: 0.0052s. Errors so far: 0.\n",
      "Processed 2200/5481 MMSIs. Avg wall time per MMSI: 0.0060s. Errors so far: 0.\n",
      "Processed 2300/5481 MMSIs. Avg wall time per MMSI: 0.0045s. Errors so far: 0.\n",
      "Processed 2400/5481 MMSIs. Avg wall time per MMSI: 0.0062s. Errors so far: 0.\n",
      "Processed 2500/5481 MMSIs. Avg wall time per MMSI: 0.0040s. Errors so far: 0.\n",
      "Processed 2600/5481 MMSIs. Avg wall time per MMSI: 0.0057s. Errors so far: 0.\n",
      "Processed 2700/5481 MMSIs. Avg wall time per MMSI: 0.0041s. Errors so far: 0.\n",
      "Processed 2800/5481 MMSIs. Avg wall time per MMSI: 0.0037s. Errors so far: 0.\n",
      "Processed 2900/5481 MMSIs. Avg wall time per MMSI: 0.0041s. Errors so far: 0.\n",
      "Processed 3000/5481 MMSIs. Avg wall time per MMSI: 0.0035s. Errors so far: 0.\n",
      "Processed 3100/5481 MMSIs. Avg wall time per MMSI: 0.0052s. Errors so far: 0.\n",
      "Processed 3200/5481 MMSIs. Avg wall time per MMSI: 0.0038s. Errors so far: 0.\n",
      "Processed 3300/5481 MMSIs. Avg wall time per MMSI: 0.0060s. Errors so far: 0.\n",
      "Processed 3400/5481 MMSIs. Avg wall time per MMSI: 0.0037s. Errors so far: 0.\n",
      "Processed 3500/5481 MMSIs. Avg wall time per MMSI: 0.0048s. Errors so far: 0.\n",
      "Processed 3600/5481 MMSIs. Avg wall time per MMSI: 0.0032s. Errors so far: 0.\n",
      "Processed 3700/5481 MMSIs. Avg wall time per MMSI: 0.0040s. Errors so far: 0.\n",
      "Processed 3800/5481 MMSIs. Avg wall time per MMSI: 0.0041s. Errors so far: 0.\n",
      "Processed 3900/5481 MMSIs. Avg wall time per MMSI: 0.0031s. Errors so far: 0.\n",
      "Processed 4000/5481 MMSIs. Avg wall time per MMSI: 0.0034s. Errors so far: 0.\n",
      "Processed 4100/5481 MMSIs. Avg wall time per MMSI: 0.0040s. Errors so far: 0.\n",
      "Processed 4200/5481 MMSIs. Avg wall time per MMSI: 0.0033s. Errors so far: 0.\n",
      "Processed 4300/5481 MMSIs. Avg wall time per MMSI: 0.0031s. Errors so far: 0.\n",
      "Processed 4400/5481 MMSIs. Avg wall time per MMSI: 0.0027s. Errors so far: 0.\n",
      "Processed 4500/5481 MMSIs. Avg wall time per MMSI: 0.0034s. Errors so far: 0.\n",
      "Processed 4600/5481 MMSIs. Avg wall time per MMSI: 0.0019s. Errors so far: 0.\n",
      "Processed 4700/5481 MMSIs. Avg wall time per MMSI: 0.0022s. Errors so far: 0.\n",
      "Processed 4800/5481 MMSIs. Avg wall time per MMSI: 0.0022s. Errors so far: 0.\n",
      "Processed 4900/5481 MMSIs. Avg wall time per MMSI: 0.0023s. Errors so far: 0.\n",
      "Processed 5000/5481 MMSIs. Avg wall time per MMSI: 0.0022s. Errors so far: 0.\n",
      "Processed 5100/5481 MMSIs. Avg wall time per MMSI: 0.0017s. Errors so far: 0.\n",
      "Processed 5200/5481 MMSIs. Avg wall time per MMSI: 0.0025s. Errors so far: 0.\n",
      "Processed 5300/5481 MMSIs. Avg wall time per MMSI: 0.0018s. Errors so far: 0.\n",
      "Processed 5400/5481 MMSIs. Avg wall time per MMSI: 0.0019s. Errors so far: 0.\n",
      "num_segments: 11416\n",
      "num_messages: 46847745\n",
      "num_discarded_filtered: 0\n",
      "num_initial_voyages: 8114\n",
      "num_voyages_after_duration_filter: 1246\n",
      "num_voyages_after_outlier_removal: 1246\n",
      "num_outlier_removal_errors: 0\n",
      "num_voyages_after_sampling: 1242\n",
      "num_sampling_errors: 4\n",
      "num_final_voyages: 787\n",
      "error_code_0: 0\n",
      "error_code_1: 0\n",
      "error_code_2: 0\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from collections import defaultdict\n",
    "# Multiprocessing requires functions to be importable from a module\n",
    "# The process_single_mmsi defined below is identical to the one imported here\n",
    "from src.preprocessing.map_reduce import process_single_mmsi as _process_single_mmsi\n",
    "from src.preprocessing.preprocessing import preprocess_mmsi_track # Preprocessing rules\n",
    "\n",
    "def process_single_mmsi(mmsi_info):\n",
    "    \"\"\" Process a single MMSI's track segments. \"\"\"\n",
    "    \n",
    "    mmsi, mmsi_dir_path, final_dir = mmsi_info\n",
    "    results = {}\n",
    "    \n",
    "    # Load all segments for this MMSI\n",
    "    all_segments = []\n",
    "    segment_files = [f for f in os.listdir(mmsi_dir_path) if f.endswith(\".pkl\") and not f.startswith(\"vessel_types_\")]\n",
    "    if not segment_files:\n",
    "        return {\"error\": f\"No segment files found for MMSI {mmsi}\",\n",
    "                \"error_code\": 0}\n",
    "    for seg_file in segment_files:\n",
    "            segment_path = os.path.join(mmsi_dir_path, seg_file)\n",
    "            track_segment = joblib.load(segment_path)\n",
    "            all_segments.append(track_segment)\n",
    "    \n",
    "    results['num_segments'] = len(all_segments)\n",
    "    \n",
    "    try: # Merge into one track\n",
    "        full_track = np.concatenate(all_segments, axis=0)\n",
    "    except ValueError as e:\n",
    "        return {\"error\": f\"Error concatenating segments for MMSI {mmsi}: {str(e)}\",\n",
    "                \"error_code\": 1}\n",
    "\n",
    "    try: # Run processing for single MMSI's track\n",
    "        processed_data, preprocessing_results = preprocess_mmsi_track(full_track)\n",
    "        results.update(preprocessing_results)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error processing track for MMSI {mmsi}: {str(e)}\",\n",
    "                \"error_code\": 2}\n",
    "    \n",
    "    if processed_data: # Save final result\n",
    "        for k, traj in processed_data.items(): # Constitues a sample\n",
    "            final_output_path = os.path.join(final_dir, f\"{mmsi}_{k}_processed.pkl\")\n",
    "            data_item = {'mmsi': mmsi, 'traj': traj}\n",
    "            joblib.dump(data_item, final_output_path, compress=3)\n",
    "            \n",
    "    return results\n",
    "    \n",
    "def reduce(final_dir: str, temp_dir: str, n_workers: int = None, chunk_size: int = 10):\n",
    "    \"\"\" Preprocess vessel trajectories by MMSI in parallel.\"\"\"\n",
    "    \n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    mmsi_folders = os.listdir(temp_dir)\n",
    "    \n",
    "    print(f\"Starting reduce phase on {len(mmsi_folders)} MMSI folders\")\n",
    "    \n",
    "    # Prepare list of (mmsi, path, output_dir) tuples for parallel processing\n",
    "    mmsi_tasks = []\n",
    "    for mmsi in mmsi_folders:\n",
    "        mmsi_dir_path = os.path.join(temp_dir, mmsi)\n",
    "        if os.path.isdir(mmsi_dir_path):\n",
    "            mmsi_tasks.append((mmsi, mmsi_dir_path, final_dir))\n",
    "    \n",
    "    results = defaultdict(int) # To count preprocessing statistics\n",
    "    \n",
    "    # Process in parallel using imap_unordered to avoid accumulating results in memory\n",
    "    t0 = time.time()\n",
    "    logging_interval = 100\n",
    "    with Pool(processes=n_workers, maxtasksperchild=min(1,1000//chunk_size)) as pool:\n",
    "        for i, result in enumerate(pool.imap_unordered(_process_single_mmsi, mmsi_tasks, chunksize=chunk_size), 1):\n",
    "            if \"error\" in result:\n",
    "                print(result[\"error\"])\n",
    "                results[f\"error_code_{result['error_code']}\"] += 1\n",
    "            else:\n",
    "                for key, value in result.items():\n",
    "                    results[key] += value\n",
    "            if i % logging_interval == 0:\n",
    "                elapsed = time.time() - t0\n",
    "                errors = sum([results[f\"error_code_{code}\"] for code in range(3)])\n",
    "                print(f\"Processed {i}/{len(mmsi_tasks)} MMSIs. Avg wall time per MMSI: {elapsed / float(i):.4f}s. Errors so far: {errors}.\")\n",
    "                t0 = time.time()\n",
    "    \n",
    "    for key, value in results.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "n_workers = cpu_count() - 1\n",
    "print(f\"Using {n_workers} workers for reduce phase.\")\n",
    "reduce(final_dir=f\"{data_dir}/processed/\", temp_dir=f\"{data_dir}/temp_dir/\", n_workers=n_workers, chunk_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ebe7c",
   "metadata": {},
   "source": [
    "### Combine vessel_types and cleanup temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b681154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup completed.\n"
     ]
    }
   ],
   "source": [
    "vessel_type_dir = f\"{data_dir}/pickle_files/vessel_types/\"\n",
    "vessel_types_combined = dict()\n",
    "vessel_type_files = [f for f in os.listdir(vessel_type_dir) if f.startswith(\"vessel_types_\") and f.endswith(\".pkl\")]\n",
    "for vt_file in vessel_type_files:\n",
    "    vt_path = os.path.join(vessel_type_dir, vt_file)\n",
    "    with open(vt_path, \"rb\") as f:\n",
    "        vt_mapping = pickle.load(f)\n",
    "        vessel_types_combined.update(vt_mapping) # In case of conflicts, later files overwrite earlier ones\n",
    "    os.remove(vt_path)\n",
    "combined_vt_path = os.path.join(f\"{data_dir}/processed/\", \"vessel_types.pkl\")\n",
    "with open(combined_vt_path, \"wb\") as f:\n",
    "    pickle.dump(vessel_types_combined, f)\n",
    "\n",
    "# Cleanup temporary directories\n",
    "for temp_dir in [f\"{data_dir}/temp_dir/\", f\"{data_dir}/pickle_files/\"]:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    \n",
    "print(\"Cleanup completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08449aa",
   "metadata": {},
   "source": [
    "## Train-validation-test split\n",
    "We use a 0.8-0.1-0.1 split for our train-, validation-, and test set, respectively. We split based on MMSI and not time, to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5347e679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 787\n",
      "Training samples: 631\n",
      "Validation samples: 78\n",
      "Test samples: 78\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from src.preprocessing.train_test_split import move_to_dir\n",
    "\n",
    "def train_test_split_tracks(data_dir, val_size=0.1, test_size=0.1, random_state=42):\n",
    "    \"\"\" Splits the dataset into training, validation, and test sets. Saves into tree subdirectories: train, val, test.\"\"\"\n",
    "    \n",
    "    all_files = [f for f in os.listdir(data_dir) if f.endswith('.pkl') and not f.startswith('vessel_types')]\n",
    "    \n",
    "    os.makedirs(os.path.join(data_dir, 'train'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(data_dir, 'val'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(data_dir, 'test'), exist_ok=True)\n",
    "    \n",
    "    total_n = len(all_files)\n",
    "    test_n = int(total_n * test_size)\n",
    "    val_n = int(total_n * val_size)\n",
    "    train_n = total_n - test_n - val_n\n",
    "    \n",
    "    random.seed(random_state)\n",
    "    \n",
    "    # Make a MMSI-aware split\n",
    "    unassigned_files = all_files.copy()\n",
    "    for partition_size, partition in zip([train_n, val_n, test_n],\n",
    "                                         ['train', 'val', 'test']):\n",
    "        running_count = 0\n",
    "        while running_count < partition_size and unassigned_files:\n",
    "            file = unassigned_files.pop(random.randint(0, len(unassigned_files)-1))\n",
    "            move_to_dir(file, data_dir, os.path.join(data_dir, partition))\n",
    "            running_count += 1\n",
    "            mmsi = file.split('_')[0]\n",
    "            additional_files_to_add = [f for f in unassigned_files if f.startswith(mmsi+'_')]\n",
    "            for af in additional_files_to_add:\n",
    "                move_to_dir(af, data_dir, os.path.join(data_dir, partition))\n",
    "                running_count += 1\n",
    "                unassigned_files.remove(af)\n",
    "    \n",
    "    return total_n, train_n, val_n, test_n\n",
    "\n",
    "total_n, train_n, val_n, test_n = train_test_split_tracks(data_dir=f\"{data_dir}/processed/\", val_size=0.1, test_size=0.1, random_state=42)\n",
    "print(f\"Total samples: {total_n}\")\n",
    "print(f\"Training samples: {train_n}\")\n",
    "print(f\"Validation samples: {val_n}\")\n",
    "print(f\"Test samples: {test_n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ddb4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.datasets import AISDataset\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AISDataset(data_dir=f\"{data_dir}/processed/train/\", device=device)\n",
    "val_dataset = AISDataset(data_dir=f\"{data_dir}/processed/val/\", device=device)\n",
    "test_dataset = AISDataset(data_dir=f\"{data_dir}/processed/test/\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190dcbc",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "TODO:\n",
    "- Train simple K-means on lat and lon\n",
    "- Train DBSCAN on lat and lon\n",
    "- Figure out optimal hyperparams (K for K-means, eps and min_samples for DBSCAN)\n",
    "- Plot clusters on map\n",
    "- Exploratory analysis based on clusters (eg. distribution of MMSI (use vessel_types.pkl mapping), number of ships in various clusters, etc)\n",
    "- Define a criteria for an initial anomaly detection algorithm based on the clusters and report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d2be9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=10, n_init=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>KMeans</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.cluster.KMeans.html\">?<span>Documentation for KMeans</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_clusters',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_clusters&nbsp;</td>\n",
       "            <td class=\"value\">10</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('init',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">init&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;k-means++&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_init',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_init&nbsp;</td>\n",
       "            <td class=\"value\">10</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">300</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">42</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('copy_x',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">copy_x&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('algorithm',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">algorithm&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;lloyd&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "KMeans(n_clusters=10, n_init=10, random_state=42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage of dataset\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "all_lat_lon = [seq[:, 0:2] for seq, _,_,_ in train_dataset] # Dataset returns (seq, seqlen, mmsi, time_start)\n",
    "\n",
    "X = torch.cat(all_lat_lon, dim=0).numpy()\n",
    "\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9eab8",
   "metadata": {},
   "source": [
    "## Further preprocess data by training a BERT encoder to encode trajectories\n",
    "- Train BERT-encoder on trajectories (use `transformers` library, use tokenizer from [TrAISformer repository](https://github.com/CIA-Oceanix/TrAISformer))\n",
    "- Generate $n$ vectors, one for each trajectory, using the BERT encoder\n",
    "- Run DBSCAN on the vectors to make new clusters\n",
    "- Perform similar exploratory analysis as with K-Means and DBSCAN\n",
    "- Reduce dimensionality to 2D using PCA or t-SNE to visualize clusters\n",
    "- Run anomaly classification using this setup and report results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915f629",
   "metadata": {},
   "source": [
    "## References\n",
    "D. Nguyen, R. Fablet. \"A Transformer Network With Sparse Augmented Data Representation and Cross Entropy Loss for AIS-Based Vessel Trajectory Prediction,\" in IEEE Access, vol. 12, pp. 21596–21609, 2024."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-tools-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
