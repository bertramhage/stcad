{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d86d38",
   "metadata": {},
   "source": [
    "# Code Implementation\n",
    "This file contains the main pipeline for the project.\n",
    "\n",
    "Additional helper functions and modules can be found under `src/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d6e81",
   "metadata": {},
   "source": [
    "## Data preprocessing using MapReduce\n",
    "The data used in this project is maritime data from automatic identification systems (AIS) obtained obtained from the [Danish Maritime Authority](http://aisdata.ais.dk/). The data is available as a csv file for each day and contains a row for each AIS message with columns such as **Timestamp**, **MMSI**, **Latitude**, **Longitude**, and more. MMSI stands for Maritime Mobile Service Identity and is a unique identifier for a vessel.\n",
    "\n",
    "Uncompressed, the data for a single day takes up around 3GB of memory and we wish to process 3 months worth of data leading to an infeasible amount of data to keep in memory at one time. However, since the data is time series data and vessel voyages often spans across days, in order to properly preprocess the data we can't process the files in isolation. Secondly, we wish to speed up the wall clock time of preprocessing by efficiently utilizing parallel processing on multiple CPU's running on DTU's High Performance Computing (HPC) cluster. This is where MapReduce comes in.\n",
    "\n",
    "### Split\n",
    "\n",
    "The preprocessing script is adapted from [CIA-Oceanix/GeoTrackNet](https://github.com/CIA-Oceanix/GeoTrackNet) and first converts each CSV file individually to dictionaries of arrays grouped by MMSI. The grouped dictionaries are saved as pickle files in a temporary directory. This is the split part of MapReduce. A simplified code this is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601663da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from src.preprocessing.csv2pkl import (map_nav_status_to_int,\n",
    "                                       convert_str_to_unix,\n",
    "                                       map_ship_type_to_int,\n",
    "                                       save_vessel_types,\n",
    "                                       filter_messages)\n",
    "\n",
    "def process_single_csv(csv_filename,\n",
    "                       input_dir,\n",
    "                       output_dir,\n",
    "                       vessel_type_dir,\n",
    "                       lat_min,\n",
    "                       lat_max,\n",
    "                       lon_min,\n",
    "                       lon_max,\n",
    "                       sog_max):\n",
    "    \n",
    "    # Define column indices\n",
    "    LAT, LON, SOG, COG, HEADING, ROT, NAV_STT, TIMESTAMP, MMSI, SHIPTYPE  = list(range(10))\n",
    "    \n",
    "    t_date_str = '-'.join(csv_filename.split('.')[0].split('-')[1:4])\n",
    "    t_min = time.mktime(time.strptime(t_date_str + ' 00:00:00', \"%Y-%m-%d %H:%M:%S\"))\n",
    "    t_max = time.mktime(time.strptime(t_date_str + ' 23:59:59', \"%Y-%m-%d %H:%M:%S\"))\n",
    "    \n",
    "    l_l_msg = [] # list of AIS messages, each row is a message (list of AIS attributes)\n",
    "    data_path = os.path.join(input_dir, csv_filename)\n",
    "    \n",
    "    with open(data_path,\"r\") as f:\n",
    "        csvReader = csv.reader(f)\n",
    "        next(csvReader) # skip the legend row\n",
    "        count = 1\n",
    "        for row in csvReader:\n",
    "            count += 1\n",
    "            try:\n",
    "                l_l_msg.append([float(row[3]), # Latitude\n",
    "                                float(row[4]), # Longitude\n",
    "                                float(row[7]), # SOG\n",
    "                                float(row[8]), # COG\n",
    "                                int(row[9]), # Heading\n",
    "                                float(row[6]), # ROT\n",
    "                                int(map_nav_status_to_int(row[5])), # Navigation status\n",
    "                                int(convert_str_to_unix(row[0])), # Timestamp\n",
    "                                int(float(row[2])), # MMSI\n",
    "                                int(map_ship_type_to_int(row[13]))]) # Ship type\n",
    "            except:\n",
    "                print(f\"Error parsing row {count} in file {csv_filename}. Skipping row.\")\n",
    "                continue\n",
    "            \n",
    "    m_msg = np.array(l_l_msg)\n",
    "        \n",
    "    if vessel_type_dir is not None:\n",
    "        save_vessel_types(m_msg, vessel_type_dir, t_date_str) # Save vessel types mapping\n",
    "\n",
    "    ## Filter messages based on min/max criteria\n",
    "    m_msg = filter_messages(m_msg, lat_min, lat_max, lon_min, lon_max, sog_max, t_min, t_max)\n",
    "\n",
    "    ## Build vessel tracks dictionary\n",
    "    Vs = dict()\n",
    "    for v_msg in m_msg:\n",
    "        mmsi = int(v_msg[MMSI])\n",
    "        if not (mmsi in list(Vs.keys())):\n",
    "            Vs[mmsi] = np.empty((0,9))\n",
    "        Vs[mmsi] = np.concatenate((Vs[mmsi], np.expand_dims(v_msg[:9],0)), axis = 0)\n",
    "    for key in Vs.keys(): # Sort each vessel's messages by timestamp\n",
    "        Vs[key] = np.array(sorted(Vs[key], key=lambda m_entry: m_entry[TIMESTAMP]))\n",
    "            \n",
    "\n",
    "    ## Save to pickle file\n",
    "    output_filename = csv_filename.replace('csv', 'pkl') \n",
    "    with open(os.path.join(output_dir,output_filename),\"wb\") as f:\n",
    "        pickle.dump(Vs,f)\n",
    "        \n",
    "LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, SOG_MAX, DURATION_MAX = 5.0, 17.0, 54.0, 59.0, 30.0, 24\n",
    "\n",
    "input_dir = 'data/files/'\n",
    "output_dir = 'data/pickle_files'\n",
    "vessel_type_dir = os.path.join(output_dir, 'vessel_types')\n",
    "\n",
    "l_csv_filename = [filename for filename in os.listdir(input_dir) if filename.endswith('.csv')]\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process csvs in parallel\n",
    "tasks = [(csv_file, input_dir, output_dir, vessel_type_dir, \n",
    "        LAT_MIN, LAT_MAX, LON_MIN, LON_MAX, SOG_MAX) \n",
    "        for csv_file in l_csv_filename]\n",
    "\n",
    "n_workers = cpu_count() - 1  # Leave 1 core free\n",
    "with Pool(processes=n_workers) as pool:\n",
    "    results = [pool.starmap(process_single_csv, tasks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1728809",
   "metadata": {},
   "source": [
    "### Mapping and shuffling\n",
    "Now that the full dataset has been chunked (split) we map each item (trajectory) based on MMSI to a MMSI directory ready for preprocessing (reduction).\n",
    "\n",
    "The resulting temporary directory has the structure:\\\n",
    "```\n",
    "data/\n",
    "└── temp_dir/\n",
    "    ├── 123456789/                      # MMSI (unique vessel identifier)\n",
    "    │   ├── chunk_0001.pkl              # Segment(s) from input_dir\n",
    "    │   ├── chunk_0002.pkl\n",
    "    │\n",
    "    ├── 987654321/\n",
    "    │   ├── chunk_0001.pkl\n",
    "    │\n",
    "    └── ...                             # One folder per MMSI\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e10036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_and_shuffle(input_dir: str, temp_dir: str):\n",
    "    \"\"\" Goes through all input files and re-sorts them by MMSI into a temporary directory. \"\"\"\n",
    "    \n",
    "    # Input files from chunking step\n",
    "    input_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".pkl\")]\n",
    "\n",
    "    for file_path in input_files:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            data_dict = pickle.load(f)\n",
    "            \n",
    "            for mmsi, track_segment in data_dict.items():\n",
    "                \n",
    "                # Create a directory for this specific MMSI\n",
    "                mmsi_dir = os.path.join(temp_dir, str(mmsi))\n",
    "                os.makedirs(mmsi_dir, exist_ok=True)\n",
    "                \n",
    "                # Save this segment into the MMSI's folder\n",
    "                # We name it after the original file to avoid collisions\n",
    "                segment_filename = os.path.basename(file_path)\n",
    "                output_path = os.path.join(mmsi_dir, segment_filename)\n",
    "                \n",
    "                with open(output_path, \"wb\") as out_f:\n",
    "                    pickle.dump(track_segment, out_f)\n",
    "                    \n",
    "temp_dir = 'data/temp_mapped'\n",
    "map_and_shuffle(input_dir=output_dir, temp_dir=temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849553f7",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "In the final step of the MapReduce algorithm, the reduction step, we apply preprocessing of the vessel trajectories. As we consider vessels' trajectories as independent from each other, and we have split and shuffled the trajectories by MMSI in the previous step, we are able to perform this step in parallel. \n",
    "\n",
    "The preprocessing includes identifying a vessels \"voyages\". We define a voyage as a contiguous sequence of AIS messages from the same vessel (possible across days), where the time interval between any two consecutive messages does not exceed two hours, and the vessel is actively moving (i.e., not moored or at anchor). See [D. Nguyen, R. Fablet](https://arxiv.org/pdf/2109.03958) for the full preprocessing rules implemented.\n",
    "\n",
    "The folder structure for the finally preprocessed files will look like:\n",
    "```\n",
    "final_processed/\n",
    "├── 123456789_0_processed.pkl            # Processed trajectory for MMSI 123456789 (segment 0)\n",
    "├── 123456789_1_processed.pkl            # (if multiple processed trajectories exist for same MMSI)\n",
    "├── 987654321_0_processed.pkl\n",
    "├── 987654321_1_processed.pkl\n",
    "└── ...\n",
    "```\n",
    "where each pickle file constitutes one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943375dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.preprocessing import preprocess_mmsi_track\n",
    "\n",
    "def process_single_mmsi(mmsi_info):\n",
    "    \"\"\" Wrapper to unpack arguments for multiprocessing.\"\"\"\n",
    "    mmsi, mmsi_dir_path, final_dir = mmsi_info\n",
    "    \n",
    "    # Load all segments for this MMSI\n",
    "    all_segments = []\n",
    "    segment_files = [f for f in os.listdir(mmsi_dir_path) if f.endswith(\".pkl\") and not f.startswith(\"vessel_types_\")]\n",
    "    if not segment_files:\n",
    "        return None\n",
    "    for seg_file in segment_files:\n",
    "            segment_path = os.path.join(mmsi_dir_path, seg_file)\n",
    "            with open(segment_path, \"rb\") as f:\n",
    "                track_segment = pickle.load(f)\n",
    "                all_segments.append(track_segment)\n",
    "    \n",
    "    # Merge into one track\n",
    "    try:\n",
    "        full_track = np.concatenate(all_segments, axis=0)\n",
    "    except ValueError:\n",
    "        print(f\"    MMSI {mmsi}: Error concatenating. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Run processing for single MMSI's track\n",
    "    processed_data = preprocess_mmsi_track(mmsi, full_track)\n",
    "    \n",
    "    # Save final result\n",
    "    if processed_data:\n",
    "        for k, traj in processed_data.items():\n",
    "            final_output_path = os.path.join(final_dir, f\"{mmsi}_{k}_processed.pkl\")\n",
    "            data_item = {'mmsi': mmsi, 'traj': traj}\n",
    "            with open(final_output_path, \"wb\") as f:\n",
    "                pickle.dump(data_item, f)\n",
    "        return True\n",
    "    return None\n",
    "    \n",
    "def reduce(final_dir: str, temp_dir: str,  n_workers: int = None):\n",
    "    \"\"\"\n",
    "    Preprocess vessel trajectories by MMSI in parallel.\n",
    "    \"\"\"\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    \n",
    "    mmsi_folders = os.listdir(temp_dir)\n",
    "    \n",
    "    # Prepare list of (mmsi, path, output_dir) tuples for parallel processing\n",
    "    mmsi_tasks = []\n",
    "    for mmsi in mmsi_folders:\n",
    "        mmsi_dir_path = os.path.join(temp_dir, mmsi)\n",
    "        if os.path.isdir(mmsi_dir_path):\n",
    "            mmsi_tasks.append((mmsi, mmsi_dir_path, final_dir))\n",
    "    \n",
    "    # Process in parallel\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        results = [pool.imap_unordered(process_single_mmsi, mmsi_tasks)]\n",
    "        \n",
    "num_workers = cpu_count() - 1  # Leave 1 core free\n",
    "final_dir = 'data/final_processed'\n",
    "reduce(final_dir=final_dir, temp_dir=temp_dir, n_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ebe7c",
   "metadata": {},
   "source": [
    "### Combine vessel_types and cleanup temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "vessel_types_combined = dict()\n",
    "vessel_type_files = [f for f in os.listdir(vessel_type_dir) if f.startswith(\"vessel_types_\") and f.endswith(\".pkl\")]\n",
    "for vt_file in vessel_type_files:\n",
    "    vt_path = os.path.join(vessel_type_dir, vt_file)\n",
    "    with open(vt_path, \"rb\") as f:\n",
    "        vt_mapping = pickle.load(f)\n",
    "        vessel_types_combined.update(vt_mapping) # In case of conflicts, later files overwrite earlier ones\n",
    "    os.remove(vt_path)\n",
    "combined_vt_path = os.path.join(final_dir, \"vessel_types.pkl\")\n",
    "with open(combined_vt_path, \"wb\") as f:\n",
    "    pickle.dump(vessel_types_combined, f)\n",
    "    \n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190dcbc",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9eab8",
   "metadata": {},
   "source": [
    "## Something \"new\"\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915f629",
   "metadata": {},
   "source": [
    "## References\n",
    "D. Nguyen, R. Fablet. \"A Transformer Network With Sparse Augmented Data Representation and Cross Entropy Loss for AIS-Based Vessel Trajectory Prediction,\" in IEEE Access, vol. 12, pp. 21596–21609, 2024."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-tools-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
