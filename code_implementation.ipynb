{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d86d38",
   "metadata": {},
   "source": [
    "# Code Implementation\n",
    "This file contains the main pipeline for the project.\n",
    "\n",
    "Additional helper functions and modules can be found under `src/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4907e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared imports\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import joblib\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "data_dir = \"data/ais/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe38586",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "The data used in this project is maritime data from automatic identification systems (AIS) obtained obtained from the [Danish Maritime Authority](http://aisdata.ais.dk/). The data is available as a csv file for each day and contains a row for each AIS message with columns such as **Timestamp**, **MMSI**, **Latitude**, **Longitude**, and more. MMSI stands for Maritime Mobile Service Identity and is a unique identifier for a vessel.\n",
    "\n",
    "For this notebook we use a subset of the data by using data only for 3 days: `2024-05-01`, `2024-05-02`, `2024-05-03`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d93f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AIS data download from 2024-05-01 to 2024-05-03 into data/ais/\n",
      "Downloading: http://aisdata.ais.dk/2024/aisdk-2024-05-01.zip\n",
      "Downloading: http://aisdata.ais.dk/2024/aisdk-2024-05-02.zip\n",
      "Downloading: http://aisdata.ais.dk/2024/aisdk-2024-05-03.zip\n",
      "\n",
      "All files downloaded successfully.\n",
      "End of download process.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def download_ais_data(from_date: date, to_date: date, destination_path: str):\n",
    "    \"\"\"Downloads and unzips AIS data for a given date range.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(destination_path):\n",
    "        os.makedirs(destination_path)\n",
    "        \n",
    "    print(f\"Starting AIS data download from {from_date} to {to_date} into {destination_path}\")\n",
    "\n",
    "    base_url = \"http://aisdata.ais.dk/\"\n",
    "    current_date = from_date\n",
    "    \n",
    "    errors = []\n",
    "    successes = 0\n",
    "    while current_date <= to_date:\n",
    "        year = current_date.strftime(\"%Y\")\n",
    "        month = current_date.strftime(\"%m\")\n",
    "        day = current_date.strftime(\"%d\")\n",
    "        \n",
    "        file_name = f\"aisdk-{year}-{month}-{day}.zip\" # Construct the file name and URL\n",
    "        file_url = f\"{base_url}{year}/{file_name}\"\n",
    "        \n",
    "        print(f\"Downloading: {file_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with io.BytesIO(response.content) as zip_buffer:\n",
    "                    with zipfile.ZipFile(zip_buffer, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(destination_path)\n",
    "                        unzipped_files = zip_ref.namelist()\n",
    "                        assert len(unzipped_files) == 1, \"Expected exactly one file in the zip archive.\"\n",
    "                        successes += 1\n",
    "                        \n",
    "            elif response.status_code == 404:\n",
    "                print(f\"Data not found for {current_date} (404 Error). Skipping.\")\n",
    "                errors.append((current_date, \"404 Not Found\"))\n",
    "            else:\n",
    "                print(f\"Failed to download {file_name}. Status code: {response.status_code}\")\n",
    "                errors.append((current_date, f\"HTTP {response.status_code}\"))\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred during download for {current_date}: {e}\")\n",
    "            errors.append((current_date, str(e)))\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"Downloaded file for {current_date} is not a valid zip file.\")\n",
    "            errors.append((current_date, \"Bad Zip File\"))\n",
    "        except AssertionError as ae:\n",
    "            print(f\"Assertion error for {current_date}: {ae}\")\n",
    "            errors.append((current_date, str(ae)))\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    if len(errors) == 0:\n",
    "        print(\"\\nAll files downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"\\nDownload succeeded for {successes}/{(successes + len(errors))} days.\")\n",
    "        print(f\"Errors encountered for the following dates:\")\n",
    "        for err_date, err_msg in errors:\n",
    "            print(f\" - {err_date}: {err_msg}\")\n",
    "    print(\"End of download process.\")\n",
    "    \n",
    "download_ais_data(date(2024, 5, 1), date(2024, 5, 3), data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d6e81",
   "metadata": {},
   "source": [
    "## Data preprocessing using MapReduce\n",
    "Uncompressed, the data for a single day takes up around 3GB of memory and we wish to process 3 months worth of data leading to an infeasible amount of data to keep in memory at one time. However, since the data is time series data and vessel voyages often spans across days, in order to properly preprocess the data we can't process the files in isolation. Secondly, we wish to speed up the wall clock time of preprocessing by efficiently utilizing parallel processing on multiple CPU's running on DTU's High Performance Computing (HPC) cluster. This is where MapReduce comes in.\n",
    "\n",
    "### Split\n",
    "We first converts each CSV file individually to dictionaries of arrays grouped by MMSI. The grouped dictionaries are saved as pickle files in a temporary directory. This is the split part of MapReduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601663da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 CSV files in data/ais/.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a994b8da6546d7b27a228bef844afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading csvs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85336ea3568a42fb9f001deca454e6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building track lists...:   0%|          | 0/15464605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5c865ca0b74b2bbd806633d8cd24ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sorting and converting to NumPy...:   0%|          | 0/3963 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2f3794dfff4c5cb8c2aca37fb12665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building track lists...:   0%|          | 0/16040042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c26dd0201b4445a92cd97616ea3f79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sorting and converting to NumPy...:   0%|          | 0/3562 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5622d2621fe44545942769f84c08a4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building track lists...:   0%|          | 0/15343098 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361b3cca28784a4ab470a708ff1757f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sorting and converting to NumPy...:   0%|          | 0/3891 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed.\n",
      "Total messages processed: 60839630\n",
      "Total messages after filtering: 46847745\n",
      "Total unique vessels: 5022\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from collections import defaultdict\n",
    "from src.preprocessing.preprocessing import LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, SPEED_MAX as SOG_MAX\n",
    "from src.preprocessing.csv2pkl import SHIP_TYPE_MAP, NAV_STT_MAP\n",
    "\n",
    "# Define column indices\n",
    "LAT, LON, SOG, COG, HEADING, ROT, NAV_STT, TIMESTAMP, MMSI, SHIPTYPE  = list(range(10))\n",
    "\n",
    "def csv2pkl(input_dir=\"data/files/\",\n",
    "            output_dir=\"data/pickle_files\"):\n",
    "    \"\"\" Converts raw AIS CSV files to dictionaries grouped by MMSI and saves them as pickle files.\"\"\"\n",
    "    \n",
    "    global LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, SOG_MAX, SHIP_TYPE_MAP, NAV_STT_MAP\n",
    "      \n",
    "    l_csv_filename = [filename for filename in os.listdir(input_dir) if filename.endswith('.csv')]\n",
    "    print(f\"Found {len(l_csv_filename)} CSV files in {input_dir}.\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    results = {file_name: {\"total_messages\": 0, \"filtered_messages\": 0} for file_name in l_csv_filename}\n",
    "\n",
    "    messages_processed = 0\n",
    "    unique_vessels = set()\n",
    "    for csv_filename in tqdm(l_csv_filename, desc=f'Reading csvs'):\n",
    "        try:\n",
    "            t_date_str = '-'.join(csv_filename.split('.')[0].split('-')[1:4])\n",
    "            t_min = time.mktime(time.strptime(t_date_str + ' 00:00:00', \"%Y-%m-%d %H:%M:%S\"))\n",
    "            t_max = time.mktime(time.strptime(t_date_str + ' 23:59:59', \"%Y-%m-%d %H:%M:%S\"))\n",
    "            \n",
    "            # Lazy load data using Polars\n",
    "            lf = pl.scan_csv(os.path.join(input_dir, csv_filename),\n",
    "                            schema_overrides={\n",
    "                                \"# Timestamp\": pl.Utf8,\n",
    "                                \"MMSI\": pl.Int64,\n",
    "                                \"Latitude\": pl.Float64,\n",
    "                                \"Longitude\": pl.Float64,\n",
    "                                \"Navigational status\": pl.Utf8,\n",
    "                                \"ROT\": pl.Float64,\n",
    "                                \"SOG\": pl.Float64,\n",
    "                                \"COG\": pl.Float64,\n",
    "                                \"Heading\": pl.Int64,\n",
    "                                \"Ship type\": pl.Utf8\n",
    "                            })\n",
    "            total_messages = lf.select(pl.len()).collect()[0,0]\n",
    "            messages_processed += total_messages\n",
    "            results[csv_filename][\"total_messages\"] = total_messages\n",
    "\n",
    "            lf = (\n",
    "                lf.with_columns(\n",
    "                    pl.col(\"# Timestamp\").str.to_datetime(\"%d/%m/%Y %H:%M:%S\").dt.epoch(\"s\").alias(\"Timestamp\"), # Convert to UNIX timestamp\n",
    "                    pl.col(\"Navigational status\").replace_strict(NAV_STT_MAP, default=15) # Map navigational status to integers\n",
    "                ).filter(\n",
    "                    (pl.col(\"Latitude\") >= LAT_MIN) & (pl.col(\"Latitude\") <= LAT_MAX) &\n",
    "                    (pl.col(\"Longitude\") >= LON_MIN) & (pl.col(\"Longitude\") <= LON_MAX) &\n",
    "                    (pl.col(\"SOG\") >= 0) & (pl.col(\"SOG\") <= SOG_MAX) &\n",
    "                    (pl.col(\"COG\") >= 0) & (pl.col(\"COG\") <= 360) &\n",
    "                    (pl.col(\"Timestamp\") >= t_min) & (pl.col(\"Timestamp\") <= t_max)\n",
    "                ).select( # Select only the 9 columns needed for the track + ship type\n",
    "                    pl.col(\"Latitude\"),\n",
    "                    pl.col(\"Longitude\"),\n",
    "                    pl.col(\"SOG\"),\n",
    "                    pl.col(\"COG\"),\n",
    "                    pl.col(\"Heading\"),\n",
    "                    pl.col(\"ROT\"),\n",
    "                    pl.col(\"Navigational status\"),\n",
    "                    pl.col(\"Timestamp\"),\n",
    "                    pl.col(\"MMSI\"),\n",
    "                    pl.col(\"Ship type\")\n",
    "                )\n",
    "            )\n",
    "                    \n",
    "            ### Vessel Type Mapping\n",
    "            vessel_type_dir = os.path.join(output_dir, \"vessel_types\")\n",
    "            os.makedirs(vessel_type_dir, exist_ok=True)\n",
    "            vt_df = (\n",
    "                lf.with_columns(\n",
    "                    pl.col(\"Ship type\").replace_strict(SHIP_TYPE_MAP, default=0)\n",
    "                )\n",
    "                .filter(pl.col(\"Ship type\") != 0) # \"Undefined\"\n",
    "                .group_by(\"MMSI\")\n",
    "                .agg(\n",
    "                    pl.col(\"Ship type\").mode().first().alias(\"VesselType\")  # If multiple use the most frequent type\n",
    "                )\n",
    "                .collect()\n",
    "            )\n",
    "            \n",
    "            unique_vessels.update(vt_df[\"MMSI\"].to_list())\n",
    "            \n",
    "            # Save vessel types mapping\n",
    "            VesselTypes = {row[0]: row[1] for row in vt_df.iter_rows()}\n",
    "            vt_output_filename = csv_filename.replace('csv', 'pkl')\n",
    "            with open(os.path.join(vessel_type_dir, vt_output_filename), \"wb\") as f:\n",
    "                pickle.dump(VesselTypes, f)\n",
    "                \n",
    "            df = lf.drop(\"Ship type\").collect() # Ship type column no longer needed\n",
    "            results[csv_filename][\"filtered_messages\"] = df.height\n",
    "            \n",
    "            # Build tracks\n",
    "            Vs_list = defaultdict(list)\n",
    "            for row_tuple in tqdm(df.iter_rows(named=False), total=len(df), desc=\"Building track lists...\", leave=False):\n",
    "                mmsi = row_tuple[MMSI] \n",
    "                Vs_list[mmsi].append(row_tuple)\n",
    "            \n",
    "            Vs = {} # Final dictionary\n",
    "            for mmsi, track_list in tqdm(Vs_list.items(), desc=\"Sorting and converting to NumPy...\", leave=False):\n",
    "                track_list.sort(key=lambda x: x[TIMESTAMP])\n",
    "                Vs[mmsi] = np.array(track_list, dtype=np.float64)\n",
    "            \n",
    "            output_filename = csv_filename.replace('csv', 'pkl') \n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            joblib.dump(Vs, output_path, compress=3)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {csv_filename}: {e}\")\n",
    "        \n",
    "    print(\"Conversion completed.\")\n",
    "    \n",
    "    total_messages = sum(info[\"total_messages\"] for info in results.values())\n",
    "    total_filtered = sum(info[\"filtered_messages\"] for info in results.values())\n",
    "    print(f\"Total messages processed: {total_messages}\")\n",
    "    print(f\"Total messages after filtering: {total_filtered}\")\n",
    "    print(f\"Total unique vessels: {len(unique_vessels)}\")\n",
    "    \n",
    "csv2pkl(input_dir=data_dir, output_dir=f\"{data_dir}/pickle_files/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1728809",
   "metadata": {},
   "source": [
    "### Mapping and shuffling\n",
    "Now that the full dataset has been chunked (split) we map each item (trajectory) based on MMSI to a MMSI directory ready for preprocessing (reduction).\n",
    "\n",
    "The resulting temporary directory has the structure:\\\n",
    "```\n",
    "data/\n",
    "└── temp_dir/\n",
    "    ├── 123456789/                      # MMSI (unique vessel identifier)\n",
    "    │   ├── chunk_0001.pkl              # Segment(s) from input_dir\n",
    "    │   ├── chunk_0002.pkl\n",
    "    │\n",
    "    ├── 987654321/\n",
    "    │   ├── chunk_0001.pkl\n",
    "    │\n",
    "    └── ...                             # One folder per MMSI\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e10036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting map and shuffle phase on 3 files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa741b552644a09a129801f8f81bf8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map and Shuffle:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map and shuffle phase completed.\n"
     ]
    }
   ],
   "source": [
    "def map_and_shuffle(input_dir: str, temp_dir: str):\n",
    "    \"\"\" Goes through all input files and re-sorts them by MMSI into a temporary directory. \"\"\"\n",
    "    \n",
    "    input_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".pkl\")] # Input files from chunking step\n",
    "    \n",
    "    print(f\"Starting map and shuffle phase on {len(input_files)} files...\")\n",
    "\n",
    "    for file_path in tqdm(input_files, desc=\"Map and Shuffle\"):\n",
    "        data_dict = joblib.load(file_path)\n",
    "            \n",
    "        for mmsi, track_segment in data_dict.items():\n",
    "            \n",
    "            # Create a directory for this specific MMSI\n",
    "            mmsi_dir = os.path.join(temp_dir, str(mmsi))\n",
    "            os.makedirs(mmsi_dir, exist_ok=True)\n",
    "            \n",
    "            # Save this segment into the MMSI's folder\n",
    "            # We name it after the original file to avoid collisions\n",
    "            segment_filename = os.path.basename(file_path)\n",
    "            output_path = os.path.join(mmsi_dir, segment_filename)\n",
    "            \n",
    "            joblib.dump(track_segment, output_path, compress=3)\n",
    "    \n",
    "    print(\"Map and shuffle phase completed.\")\n",
    "\n",
    "map_and_shuffle(input_dir=f\"{data_dir}/pickle_files/\", temp_dir=f\"{data_dir}/temp_dir/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849553f7",
   "metadata": {},
   "source": [
    "### Reduce (in parallel)\n",
    "In the final step of the MapReduce algorithm, the reduction step, we apply preprocessing of the vessel trajectories. As we consider vessels' trajectories as independent from each other, and we have split and shuffled the trajectories by MMSI in the previous step, we are able to perform this step in parallel. \n",
    "\n",
    "The preprocessing includes identifying a vessels \"voyages\". We define a voyage as a contiguous sequence of AIS messages from the same vessel (possible across days), where the time interval between any two consecutive messages does not exceed two hours, and the vessel is actively moving (i.e., not moored or at anchor). See [D. Nguyen, R. Fablet](https://arxiv.org/pdf/2109.03958) for the full preprocessing rules implemented.\n",
    "\n",
    "The folder structure for the finally preprocessed files will look like:\n",
    "```\n",
    "processed/\n",
    "├── 123456789_0_processed.pkl            # Processed trajectory for MMSI 123456789 (segment 0)\n",
    "├── 123456789_1_processed.pkl            # (if multiple processed trajectories exist for same MMSI)\n",
    "├── 987654321_0_processed.pkl\n",
    "├── 987654321_1_processed.pkl\n",
    "└── ...\n",
    "```\n",
    "where each pickle file constitutes one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943375dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting reduce phase on 5481 MMSI folders\n",
      "Processed 100/5481 MMSIs. Avg time per MMSI: 0.0740s. Errors so far: 0.\n",
      "Processed 200/5481 MMSIs. Avg time per MMSI: 0.0317s. Errors so far: 0.\n",
      "Processed 300/5481 MMSIs. Avg time per MMSI: 0.0138s. Errors so far: 0.\n",
      "Processed 400/5481 MMSIs. Avg time per MMSI: 0.0159s. Errors so far: 0.\n",
      "Processed 500/5481 MMSIs. Avg time per MMSI: 0.0130s. Errors so far: 0.\n",
      "Processed 600/5481 MMSIs. Avg time per MMSI: 0.0062s. Errors so far: 0.\n",
      "Processed 700/5481 MMSIs. Avg time per MMSI: 0.0107s. Errors so far: 0.\n",
      "Processed 800/5481 MMSIs. Avg time per MMSI: 0.0129s. Errors so far: 0.\n",
      "Processed 900/5481 MMSIs. Avg time per MMSI: 0.0136s. Errors so far: 0.\n",
      "Processed 1000/5481 MMSIs. Avg time per MMSI: 0.0057s. Errors so far: 0.\n",
      "Processed 1100/5481 MMSIs. Avg time per MMSI: 0.0046s. Errors so far: 0.\n",
      "Processed 1200/5481 MMSIs. Avg time per MMSI: 0.0045s. Errors so far: 0.\n",
      "Processed 1300/5481 MMSIs. Avg time per MMSI: 0.0035s. Errors so far: 0.\n",
      "Processed 1400/5481 MMSIs. Avg time per MMSI: 0.0049s. Errors so far: 0.\n",
      "Processed 1500/5481 MMSIs. Avg time per MMSI: 0.0038s. Errors so far: 0.\n",
      "Processed 1600/5481 MMSIs. Avg time per MMSI: 0.0038s. Errors so far: 0.\n",
      "Processed 1700/5481 MMSIs. Avg time per MMSI: 0.0051s. Errors so far: 0.\n",
      "Processed 1800/5481 MMSIs. Avg time per MMSI: 0.0039s. Errors so far: 0.\n",
      "Processed 1900/5481 MMSIs. Avg time per MMSI: 0.0035s. Errors so far: 0.\n",
      "Processed 2000/5481 MMSIs. Avg time per MMSI: 0.0041s. Errors so far: 0.\n",
      "Processed 2100/5481 MMSIs. Avg time per MMSI: 0.0034s. Errors so far: 0.\n",
      "Processed 2200/5481 MMSIs. Avg time per MMSI: 0.0018s. Errors so far: 0.\n",
      "Processed 2300/5481 MMSIs. Avg time per MMSI: 0.0025s. Errors so far: 0.\n",
      "Processed 2400/5481 MMSIs. Avg time per MMSI: 0.0027s. Errors so far: 0.\n",
      "Processed 2500/5481 MMSIs. Avg time per MMSI: 0.0017s. Errors so far: 0.\n",
      "Processed 2600/5481 MMSIs. Avg time per MMSI: 0.0026s. Errors so far: 0.\n",
      "Processed 2700/5481 MMSIs. Avg time per MMSI: 0.0028s. Errors so far: 0.\n",
      "Processed 2800/5481 MMSIs. Avg time per MMSI: 0.0018s. Errors so far: 0.\n",
      "Processed 2900/5481 MMSIs. Avg time per MMSI: 0.0029s. Errors so far: 0.\n",
      "Processed 3000/5481 MMSIs. Avg time per MMSI: 0.0025s. Errors so far: 0.\n",
      "Processed 3100/5481 MMSIs. Avg time per MMSI: 0.0025s. Errors so far: 0.\n",
      "Processed 3200/5481 MMSIs. Avg time per MMSI: 0.0026s. Errors so far: 0.\n",
      "Processed 3300/5481 MMSIs. Avg time per MMSI: 0.0017s. Errors so far: 0.\n",
      "Processed 3400/5481 MMSIs. Avg time per MMSI: 0.0028s. Errors so far: 0.\n",
      "Processed 3500/5481 MMSIs. Avg time per MMSI: 0.0016s. Errors so far: 0.\n",
      "Processed 3600/5481 MMSIs. Avg time per MMSI: 0.0018s. Errors so far: 0.\n",
      "Processed 3700/5481 MMSIs. Avg time per MMSI: 0.0013s. Errors so far: 0.\n",
      "Processed 3800/5481 MMSIs. Avg time per MMSI: 0.0014s. Errors so far: 0.\n",
      "Processed 3900/5481 MMSIs. Avg time per MMSI: 0.0026s. Errors so far: 0.\n",
      "Processed 4000/5481 MMSIs. Avg time per MMSI: 0.0010s. Errors so far: 0.\n",
      "Processed 4100/5481 MMSIs. Avg time per MMSI: 0.0017s. Errors so far: 0.\n",
      "Processed 4200/5481 MMSIs. Avg time per MMSI: 0.0017s. Errors so far: 0.\n",
      "Processed 4300/5481 MMSIs. Avg time per MMSI: 0.0013s. Errors so far: 0.\n",
      "Processed 4400/5481 MMSIs. Avg time per MMSI: 0.0013s. Errors so far: 0.\n",
      "Processed 4500/5481 MMSIs. Avg time per MMSI: 0.0014s. Errors so far: 0.\n",
      "Processed 4600/5481 MMSIs. Avg time per MMSI: 0.0010s. Errors so far: 0.\n",
      "Processed 4700/5481 MMSIs. Avg time per MMSI: 0.0013s. Errors so far: 0.\n",
      "Processed 4800/5481 MMSIs. Avg time per MMSI: 0.0014s. Errors so far: 0.\n",
      "Processed 4900/5481 MMSIs. Avg time per MMSI: 0.0011s. Errors so far: 0.\n",
      "Processed 5000/5481 MMSIs. Avg time per MMSI: 0.0023s. Errors so far: 0.\n",
      "Processed 5100/5481 MMSIs. Avg time per MMSI: 0.0024s. Errors so far: 0.\n",
      "Processed 5200/5481 MMSIs. Avg time per MMSI: 0.0015s. Errors so far: 0.\n",
      "Processed 5300/5481 MMSIs. Avg time per MMSI: 0.0009s. Errors so far: 0.\n",
      "Processed 5400/5481 MMSIs. Avg time per MMSI: 0.0009s. Errors so far: 0.\n",
      "num_segments: 11416\n",
      "num_messages: 46847745\n",
      "num_discarded_filtered: 0\n",
      "num_initial_voyages: 8114\n",
      "num_voyages_after_duration_filter: 1246\n",
      "num_voyages_after_outlier_removal: 1246\n",
      "num_outlier_removal_errors: 0\n",
      "num_voyages_after_sampling: 1242\n",
      "num_sampling_errors: 4\n",
      "num_final_voyages: 787\n",
      "error_code_0: 0\n",
      "error_code_1: 0\n",
      "error_code_2: 0\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from collections import defaultdict\n",
    "# Multiprocessing requires functions to be importable from a module\n",
    "# The process_single_mmsi defined below is identical to the one imported here\n",
    "from src.preprocessing.map_reduce import process_single_mmsi as _process_single_mmsi\n",
    "from src.preprocessing.preprocessing import preprocess_mmsi_track # Preprocessing rules\n",
    "\n",
    "def process_single_mmsi(mmsi_info):\n",
    "    \"\"\" Process a single MMSI's track segments. \"\"\"\n",
    "    \n",
    "    mmsi, mmsi_dir_path, final_dir = mmsi_info\n",
    "    results = {}\n",
    "    \n",
    "    # Load all segments for this MMSI\n",
    "    all_segments = []\n",
    "    segment_files = [f for f in os.listdir(mmsi_dir_path) if f.endswith(\".pkl\") and not f.startswith(\"vessel_types_\")]\n",
    "    if not segment_files:\n",
    "        return {\"error\": f\"No segment files found for MMSI {mmsi}\",\n",
    "                \"error_code\": 0}\n",
    "    for seg_file in segment_files:\n",
    "            segment_path = os.path.join(mmsi_dir_path, seg_file)\n",
    "            track_segment = joblib.load(segment_path)\n",
    "            all_segments.append(track_segment)\n",
    "    \n",
    "    results['num_segments'] = len(all_segments)\n",
    "    \n",
    "    try: # Merge into one track\n",
    "        full_track = np.concatenate(all_segments, axis=0)\n",
    "    except ValueError as e:\n",
    "        return {\"error\": f\"Error concatenating segments for MMSI {mmsi}: {str(e)}\",\n",
    "                \"error_code\": 1}\n",
    "\n",
    "    try: # Run processing for single MMSI's track\n",
    "        processed_data, preprocessing_results = preprocess_mmsi_track(full_track)\n",
    "        results.update(preprocessing_results)\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error processing track for MMSI {mmsi}: {str(e)}\",\n",
    "                \"error_code\": 2}\n",
    "    \n",
    "    if processed_data: # Save final result\n",
    "        for k, traj in processed_data.items(): # Constitues a sample\n",
    "            final_output_path = os.path.join(final_dir, f\"{mmsi}_{k}_processed.pkl\")\n",
    "            data_item = {'mmsi': mmsi, 'traj': traj}\n",
    "            joblib.dump(data_item, final_output_path, compress=3)\n",
    "            \n",
    "    return results\n",
    "    \n",
    "def reduce(final_dir: str, temp_dir: str, n_workers: int = None, chunk_size: int = 10):\n",
    "    \"\"\" Preprocess vessel trajectories by MMSI in parallel.\"\"\"\n",
    "    \n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    mmsi_folders = os.listdir(temp_dir)\n",
    "    \n",
    "    print(f\"Starting reduce phase on {len(mmsi_folders)} MMSI folders\")\n",
    "    \n",
    "    # Prepare list of (mmsi, path, output_dir) tuples for parallel processing\n",
    "    mmsi_tasks = []\n",
    "    for mmsi in mmsi_folders:\n",
    "        mmsi_dir_path = os.path.join(temp_dir, mmsi)\n",
    "        if os.path.isdir(mmsi_dir_path):\n",
    "            mmsi_tasks.append((mmsi, mmsi_dir_path, final_dir))\n",
    "    \n",
    "    results = defaultdict(int) # To count preprocessing statistics\n",
    "    \n",
    "    # Process in parallel using imap_unordered to avoid accumulating results in memory\n",
    "    t0 = time.time()\n",
    "    logging_interval = 100\n",
    "    with Pool(processes=n_workers, maxtasksperchild=min(1,1000//chunk_size)) as pool:\n",
    "        for i, result in enumerate(pool.imap_unordered(_process_single_mmsi, mmsi_tasks, chunksize=chunk_size), 1):\n",
    "            if \"error\" in result:\n",
    "                print(result[\"error\"])\n",
    "                results[f\"error_code_{result['error_code']}\"] += 1\n",
    "            else:\n",
    "                for key, value in result.items():\n",
    "                    results[key] += value\n",
    "            if i % logging_interval == 0:\n",
    "                elapsed = time.time() - t0\n",
    "                errors = sum([results[f\"error_code_{code}\"] for code in range(3)])\n",
    "                print(f\"Processed {i}/{len(mmsi_tasks)} MMSIs. Avg wall time per MMSI: {elapsed / float(i):.4f}s. Errors so far: {errors}.\")\n",
    "                t0 = time.time()\n",
    "    \n",
    "    for key, value in results.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "n_workers = cpu_count() - 1\n",
    "reduce(final_dir=f\"{data_dir}/processed/\", temp_dir=f\"{data_dir}/temp_dir/\", n_workers=n_workers, chunk_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ebe7c",
   "metadata": {},
   "source": [
    "### Combine vessel_types and cleanup temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b681154b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup completed.\n"
     ]
    }
   ],
   "source": [
    "vessel_type_dir = f\"{data_dir}/pickle_files/vessel_types/\"\n",
    "vessel_types_combined = dict()\n",
    "vessel_type_files = [f for f in os.listdir(vessel_type_dir) if f.startswith(\"vessel_types_\") and f.endswith(\".pkl\")]\n",
    "for vt_file in vessel_type_files:\n",
    "    vt_path = os.path.join(vessel_type_dir, vt_file)\n",
    "    with open(vt_path, \"rb\") as f:\n",
    "        vt_mapping = pickle.load(f)\n",
    "        vessel_types_combined.update(vt_mapping) # In case of conflicts, later files overwrite earlier ones\n",
    "    os.remove(vt_path)\n",
    "combined_vt_path = os.path.join(f\"{data_dir}/processed/\", \"vessel_types.pkl\")\n",
    "with open(combined_vt_path, \"wb\") as f:\n",
    "    pickle.dump(vessel_types_combined, f)\n",
    "\n",
    "# Cleanup temporary directories\n",
    "for temp_dir in [f\"{data_dir}/temp_dir/\", f\"{data_dir}/pickle_files/\"]:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    \n",
    "print(\"Cleanup completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190dcbc",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9eab8",
   "metadata": {},
   "source": [
    "## Something \"new\"\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915f629",
   "metadata": {},
   "source": [
    "## References\n",
    "D. Nguyen, R. Fablet. \"A Transformer Network With Sparse Augmented Data Representation and Cross Entropy Loss for AIS-Based Vessel Trajectory Prediction,\" in IEEE Access, vol. 12, pp. 21596–21609, 2024."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-tools-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
