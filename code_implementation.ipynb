{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d86d38",
   "metadata": {},
   "source": [
    "# Code Implementation\n",
    "This file contains the main pipeline for the project.\n",
    "\n",
    "Additional helper functions and modules can be found under `src/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe38586",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "The raw data is available from [aisdata.ais.dk/](http://aisdata.ais.dk/). \\\n",
    "For this notebook we use a subset of the data by using data only for 3 days: `2024-05-01`, `2024-05-02`, `2024-05-03`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d93f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting AIS data download from 2024-05-01 to 2024-05-03 into data/ais/\n",
      "Downloading: http://aisdata.ais.dk/2024/aisdk-2024-05-01.zip\n",
      "Downloading: http://aisdata.ais.dk/2024/aisdk-2024-05-02.zip\n",
      "Downloading: http://aisdata.ais.dk/2024/aisdk-2024-05-03.zip\n",
      "\n",
      "All files downloaded successfully.\n",
      "End of download process.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "from datetime import date, timedelta\n",
    "\n",
    "def download_ais_data(from_date: date, to_date: date, destination_path: str):\n",
    "    \"\"\"Downloads and unzips AIS data for a given date range.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(destination_path):\n",
    "        os.makedirs(destination_path)\n",
    "        \n",
    "    print(f\"Starting AIS data download from {from_date} to {to_date} into {destination_path}\")\n",
    "\n",
    "    base_url = \"http://aisdata.ais.dk/\"\n",
    "    current_date = from_date\n",
    "    \n",
    "    errors = []\n",
    "    successes = 0\n",
    "    while current_date <= to_date:\n",
    "        year = current_date.strftime(\"%Y\")\n",
    "        month = current_date.strftime(\"%m\")\n",
    "        day = current_date.strftime(\"%d\")\n",
    "        \n",
    "        file_name = f\"aisdk-{year}-{month}-{day}.zip\" # Construct the file name and URL\n",
    "        file_url = f\"{base_url}{year}/{file_name}\"\n",
    "        \n",
    "        print(f\"Downloading: {file_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with io.BytesIO(response.content) as zip_buffer:\n",
    "                    with zipfile.ZipFile(zip_buffer, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(destination_path)\n",
    "                        unzipped_files = zip_ref.namelist()\n",
    "                        assert len(unzipped_files) == 1, \"Expected exactly one file in the zip archive.\"\n",
    "                        successes += 1\n",
    "                        \n",
    "            elif response.status_code == 404:\n",
    "                print(f\"Data not found for {current_date} (404 Error). Skipping.\")\n",
    "                errors.append((current_date, \"404 Not Found\"))\n",
    "            else:\n",
    "                print(f\"Failed to download {file_name}. Status code: {response.status_code}\")\n",
    "                errors.append((current_date, f\"HTTP {response.status_code}\"))\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"An error occurred during download for {current_date}: {e}\")\n",
    "            errors.append((current_date, str(e)))\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"Downloaded file for {current_date} is not a valid zip file.\")\n",
    "            errors.append((current_date, \"Bad Zip File\"))\n",
    "        except AssertionError as ae:\n",
    "            print(f\"Assertion error for {current_date}: {ae}\")\n",
    "            errors.append((current_date, str(ae)))\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    if len(errors) == 0:\n",
    "        print(\"\\nAll files downloaded successfully.\")\n",
    "    else:\n",
    "        print(f\"\\nDownload succeeded for {successes}/{(successes + len(errors))} days.\")\n",
    "        print(f\"Errors encountered for the following dates:\")\n",
    "        for err_date, err_msg in errors:\n",
    "            print(f\" - {err_date}: {err_msg}\")\n",
    "    print(\"End of download process.\")\n",
    "    \n",
    "download_ais_data(date(2024, 5, 1), date(2024, 5, 3), 'data/ais/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471d6e81",
   "metadata": {},
   "source": [
    "## Data preprocessing using MapReduce\n",
    "The data used in this project is maritime data from automatic identification systems (AIS) obtained obtained from the [Danish Maritime Authority](http://aisdata.ais.dk/). The data is available as a csv file for each day and contains a row for each AIS message with columns such as **Timestamp**, **MMSI**, **Latitude**, **Longitude**, and more. MMSI stands for Maritime Mobile Service Identity and is a unique identifier for a vessel.\n",
    "\n",
    "Uncompressed, the data for a single day takes up around 3GB of memory and we wish to process 3 months worth of data leading to an infeasible amount of data to keep in memory at one time. However, since the data is time series data and vessel voyages often spans across days, in order to properly preprocess the data we can't process the files in isolation. Secondly, we wish to speed up the wall clock time of preprocessing by efficiently utilizing parallel processing on multiple CPU's running on DTU's High Performance Computing (HPC) cluster. This is where MapReduce comes in.\n",
    "\n",
    "### Split\n",
    "\n",
    "The preprocessing script is adapted from [CIA-Oceanix/GeoTrackNet](https://github.com/CIA-Oceanix/GeoTrackNet) and first converts each CSV file individually to dictionaries of arrays grouped by MMSI. The grouped dictionaries are saved as pickle files in a temporary directory. This is the split part of MapReduce. A simplified code this is presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601663da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 CSV files in data/ais/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building track lists...: 100%|██████████| 15464605/15464605 [00:16<00:00, 943790.46it/s]\n",
      "Sorting and converting to NumPy...:  22%|██▏       | 878/3963 [00:16<00:58, 52.62it/s]\n",
      "Reading csvs:  33%|███▎      | 1/3 [00:48<01:37, 48.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file aisdk-2024-05-03.csv: Unable to allocate 1.56 MiB for an array with shape (22770, 9) and data type float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading csvs:  33%|███▎      | 1/3 [01:00<02:01, 60.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "  File \"/tmp/ipykernel_719712/705180081.py\", line 136, in <module>\n",
      "  File \"/tmp/ipykernel_719712/705180081.py\", line 50, in csv2pkl\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/polars/_utils/deprecation.py\", line 97, in wrapper\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/polars/lazyframe/opt_flags.py\", line 328, in wrapper\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/polars/lazyframe/frame.py\", line 2422, in collect\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2194, in showtraceback\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1188, in structured_traceback\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1059, in structured_traceback\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 867, in structured_traceback\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 752, in format_exception_as_a_whole\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 832, in get_records\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/IPython/core/tbtools.py\", line 63, in get_line_number_of_frame\n",
      "  File \"/zhome/ea/6/187439/computational-tools-project/.venv/lib/python3.11/site-packages/IPython/core/tbtools.py\", line 33, in count_lines_in_py_file\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import polars as pl\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "from src.preprocessing.preprocessing import LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, SPEED_MAX as SOG_MAX\n",
    "from src.preprocessing.csv2pkl import SHIP_TYPE_MAP, NAV_STT_MAP\n",
    "\n",
    "# Define column indices\n",
    "LAT, LON, SOG, COG, HEADING, ROT, NAV_STT, TIMESTAMP, MMSI, SHIPTYPE  = list(range(10))\n",
    "\n",
    "def csv2pkl(input_dir=\"data/files/\",\n",
    "            output_dir=\"data/pickle_files\"):\n",
    "    \n",
    "    global LON_MIN, LON_MAX, LAT_MIN, LAT_MAX, SOG_MAX, SHIP_TYPE_MAP, NAV_STT_MAP\n",
    "      \n",
    "    l_csv_filename = [filename for filename in os.listdir(input_dir) if filename.endswith('.csv')]\n",
    "    print(f\"Found {len(l_csv_filename)} CSV files in {input_dir}.\")\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    results = {file_name: {\"total_messages\": 0, \"filtered_messages\": 0} for file_name in l_csv_filename}\n",
    "\n",
    "    messages_processed = 0\n",
    "    unique_vessels = set()\n",
    "    for csv_filename in tqdm(l_csv_filename, desc=f'Reading csvs'):\n",
    "        try:\n",
    "            t_date_str = '-'.join(csv_filename.split('.')[0].split('-')[1:4])\n",
    "            t_min = time.mktime(time.strptime(t_date_str + ' 00:00:00', \"%Y-%m-%d %H:%M:%S\"))\n",
    "            t_max = time.mktime(time.strptime(t_date_str + ' 23:59:59', \"%Y-%m-%d %H:%M:%S\"))\n",
    "            \n",
    "            # Lazy load data using Polars\n",
    "            lf = pl.scan_csv(os.path.join(input_dir, csv_filename),\n",
    "                            schema_overrides={\n",
    "                                \"# Timestamp\": pl.Utf8,\n",
    "                                \"MMSI\": pl.Int64,\n",
    "                                \"Latitude\": pl.Float64,\n",
    "                                \"Longitude\": pl.Float64,\n",
    "                                \"Navigational status\": pl.Utf8,\n",
    "                                \"ROT\": pl.Float64,\n",
    "                                \"SOG\": pl.Float64,\n",
    "                                \"COG\": pl.Float64,\n",
    "                                \"Heading\": pl.Int64,\n",
    "                                \"Ship type\": pl.Utf8\n",
    "                            })\n",
    "            total_messages = lf.select(pl.len()).collect()[0,0]\n",
    "            messages_processed += total_messages\n",
    "            results[csv_filename][\"total_messages\"] = total_messages\n",
    "\n",
    "            lf = (\n",
    "                lf.with_columns(\n",
    "                    pl.col(\"# Timestamp\").str.to_datetime(\"%d/%m/%Y %H:%M:%S\").dt.epoch(\"s\").alias(\"Timestamp\"), # Convert to UNIX timestamp\n",
    "                    pl.col(\"Navigational status\").replace_strict(NAV_STT_MAP, default=15) # Map navigational status to integers\n",
    "                ).filter(\n",
    "                    (pl.col(\"Latitude\") >= LAT_MIN) & (pl.col(\"Latitude\") <= LAT_MAX) &\n",
    "                    (pl.col(\"Longitude\") >= LON_MIN) & (pl.col(\"Longitude\") <= LON_MAX) &\n",
    "                    (pl.col(\"SOG\") >= 0) & (pl.col(\"SOG\") <= SOG_MAX) &\n",
    "                    (pl.col(\"COG\") >= 0) & (pl.col(\"COG\") <= 360) &\n",
    "                    (pl.col(\"Timestamp\") >= t_min) & (pl.col(\"Timestamp\") <= t_max)\n",
    "                ).select( # Select only the 9 columns needed for the track + ship type\n",
    "                    pl.col(\"Latitude\"),\n",
    "                    pl.col(\"Longitude\"),\n",
    "                    pl.col(\"SOG\"),\n",
    "                    pl.col(\"COG\"),\n",
    "                    pl.col(\"Heading\"),\n",
    "                    pl.col(\"ROT\"),\n",
    "                    pl.col(\"Navigational status\"),\n",
    "                    pl.col(\"Timestamp\"),\n",
    "                    pl.col(\"MMSI\"),\n",
    "                    pl.col(\"Ship type\")\n",
    "                )\n",
    "            )\n",
    "                    \n",
    "            ### Vessel Type Mapping\n",
    "            vessel_type_dir = os.path.join(output_dir, \"vessel_types\")\n",
    "            os.makedirs(vessel_type_dir, exist_ok=True)\n",
    "            vt_df = (\n",
    "                lf.with_columns(\n",
    "                    pl.col(\"Ship type\").replace_strict(SHIP_TYPE_MAP, default=0)\n",
    "                )\n",
    "                .filter(pl.col(\"Ship type\") != 0) # \"Undefined\"\n",
    "                .group_by(\"MMSI\")\n",
    "                .agg(\n",
    "                    pl.col(\"Ship type\").mode().first().alias(\"VesselType\")  # If multiple use the most frequent type\n",
    "                )\n",
    "                .collect()\n",
    "            )\n",
    "            \n",
    "            unique_vessels.update(vt_df[\"MMSI\"].to_list())\n",
    "            \n",
    "            # Save vessel types mapping\n",
    "            VesselTypes = {row[0]: row[1] for row in vt_df.iter_rows()}\n",
    "            vt_output_filename = csv_filename.replace('csv', 'pkl')\n",
    "            with open(os.path.join(vessel_type_dir, vt_output_filename), \"wb\") as f:\n",
    "                pickle.dump(VesselTypes, f)\n",
    "                \n",
    "            df = lf.drop(\"Ship type\").collect() # Ship type column no longer needed\n",
    "            results[csv_filename][\"filtered_messages\"] = df.height\n",
    "            \n",
    "            # Build tracks\n",
    "            Vs_list = defaultdict(list)\n",
    "            for row_tuple in tqdm(df.iter_rows(named=False), total=len(df), desc=\"Building track lists...\"):\n",
    "                mmsi = row_tuple[MMSI] \n",
    "                Vs_list[mmsi].append(row_tuple)\n",
    "                \n",
    "            del df # Free memory\n",
    "            \n",
    "            Vs = {} # Final dictionary\n",
    "            for mmsi, track_list in tqdm(Vs_list.items(), desc=\"Sorting and converting to NumPy...\"):\n",
    "                track_list.sort(key=lambda x: x[TIMESTAMP])\n",
    "                Vs[mmsi] = np.array(track_list, dtype=np.float64)\n",
    "\n",
    "            del Vs_list # Free memory\n",
    "            \n",
    "            output_filename = csv_filename.replace('csv', 'pkl') \n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            joblib.dump(Vs, output_path, compress=3)\n",
    "\n",
    "            del Vs  # Free memory\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {csv_filename}: {e}\")\n",
    "        \n",
    "    print(\"Conversion completed.\")\n",
    "    \n",
    "    total_messages = sum(info[\"total_messages\"] for info in results.values())\n",
    "    total_filtered = sum(info[\"filtered_messages\"] for info in results.values())\n",
    "    print(f\"Total messages processed: {total_messages}\")\n",
    "    print(f\"Total messages after filtering: {total_filtered}\")\n",
    "    print(f\"Total unique vessels: {len(unique_vessels)}\")\n",
    "    \n",
    "csv2pkl(input_dir=\"data/ais/\", output_dir=\"data/ais/pickle_files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19dcb847",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[32m1\u001b[39m==\u001b[32m2\u001b[39m \u001b[38;5;66;03m# Dont run\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "assert 1==2 # Dont run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1728809",
   "metadata": {},
   "source": [
    "### Mapping and shuffling\n",
    "Now that the full dataset has been chunked (split) we map each item (trajectory) based on MMSI to a MMSI directory ready for preprocessing (reduction).\n",
    "\n",
    "The resulting temporary directory has the structure:\\\n",
    "```\n",
    "data/\n",
    "└── temp_dir/\n",
    "    ├── 123456789/                      # MMSI (unique vessel identifier)\n",
    "    │   ├── chunk_0001.pkl              # Segment(s) from input_dir\n",
    "    │   ├── chunk_0002.pkl\n",
    "    │\n",
    "    ├── 987654321/\n",
    "    │   ├── chunk_0001.pkl\n",
    "    │\n",
    "    └── ...                             # One folder per MMSI\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e10036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_and_shuffle(input_dir: str, temp_dir: str):\n",
    "    \"\"\" Goes through all input files and re-sorts them by MMSI into a temporary directory. \"\"\"\n",
    "    \n",
    "    # Input files from chunking step\n",
    "    input_files = [os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".pkl\")]\n",
    "\n",
    "    for file_path in input_files:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            data_dict = pickle.load(f)\n",
    "            \n",
    "            for mmsi, track_segment in data_dict.items():\n",
    "                \n",
    "                # Create a directory for this specific MMSI\n",
    "                mmsi_dir = os.path.join(temp_dir, str(mmsi))\n",
    "                os.makedirs(mmsi_dir, exist_ok=True)\n",
    "                \n",
    "                # Save this segment into the MMSI's folder\n",
    "                # We name it after the original file to avoid collisions\n",
    "                segment_filename = os.path.basename(file_path)\n",
    "                output_path = os.path.join(mmsi_dir, segment_filename)\n",
    "                \n",
    "                with open(output_path, \"wb\") as out_f:\n",
    "                    pickle.dump(track_segment, out_f)\n",
    "                    \n",
    "temp_dir = 'data/temp_mapped'\n",
    "map_and_shuffle(input_dir=output_dir, temp_dir=temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849553f7",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "In the final step of the MapReduce algorithm, the reduction step, we apply preprocessing of the vessel trajectories. As we consider vessels' trajectories as independent from each other, and we have split and shuffled the trajectories by MMSI in the previous step, we are able to perform this step in parallel. \n",
    "\n",
    "The preprocessing includes identifying a vessels \"voyages\". We define a voyage as a contiguous sequence of AIS messages from the same vessel (possible across days), where the time interval between any two consecutive messages does not exceed two hours, and the vessel is actively moving (i.e., not moored or at anchor). See [D. Nguyen, R. Fablet](https://arxiv.org/pdf/2109.03958) for the full preprocessing rules implemented.\n",
    "\n",
    "The folder structure for the finally preprocessed files will look like:\n",
    "```\n",
    "final_processed/\n",
    "├── 123456789_0_processed.pkl            # Processed trajectory for MMSI 123456789 (segment 0)\n",
    "├── 123456789_1_processed.pkl            # (if multiple processed trajectories exist for same MMSI)\n",
    "├── 987654321_0_processed.pkl\n",
    "├── 987654321_1_processed.pkl\n",
    "└── ...\n",
    "```\n",
    "where each pickle file constitutes one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943375dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.preprocessing import preprocess_mmsi_track\n",
    "\n",
    "def process_single_mmsi(mmsi_info):\n",
    "    \"\"\" Wrapper to unpack arguments for multiprocessing.\"\"\"\n",
    "    mmsi, mmsi_dir_path, final_dir = mmsi_info\n",
    "    \n",
    "    # Load all segments for this MMSI\n",
    "    all_segments = []\n",
    "    segment_files = [f for f in os.listdir(mmsi_dir_path) if f.endswith(\".pkl\") and not f.startswith(\"vessel_types_\")]\n",
    "    if not segment_files:\n",
    "        return None\n",
    "    for seg_file in segment_files:\n",
    "            segment_path = os.path.join(mmsi_dir_path, seg_file)\n",
    "            with open(segment_path, \"rb\") as f:\n",
    "                track_segment = pickle.load(f)\n",
    "                all_segments.append(track_segment)\n",
    "    \n",
    "    # Merge into one track\n",
    "    try:\n",
    "        full_track = np.concatenate(all_segments, axis=0)\n",
    "    except ValueError:\n",
    "        print(f\"    MMSI {mmsi}: Error concatenating. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # Run processing for single MMSI's track\n",
    "    processed_data = preprocess_mmsi_track(mmsi, full_track)\n",
    "    \n",
    "    # Save final result\n",
    "    if processed_data:\n",
    "        for k, traj in processed_data.items():\n",
    "            final_output_path = os.path.join(final_dir, f\"{mmsi}_{k}_processed.pkl\")\n",
    "            data_item = {'mmsi': mmsi, 'traj': traj}\n",
    "            with open(final_output_path, \"wb\") as f:\n",
    "                pickle.dump(data_item, f)\n",
    "        return True\n",
    "    return None\n",
    "    \n",
    "def reduce(final_dir: str, temp_dir: str,  n_workers: int = None):\n",
    "    \"\"\"\n",
    "    Preprocess vessel trajectories by MMSI in parallel.\n",
    "    \"\"\"\n",
    "    os.makedirs(final_dir, exist_ok=True)\n",
    "    \n",
    "    mmsi_folders = os.listdir(temp_dir)\n",
    "    \n",
    "    # Prepare list of (mmsi, path, output_dir) tuples for parallel processing\n",
    "    mmsi_tasks = []\n",
    "    for mmsi in mmsi_folders:\n",
    "        mmsi_dir_path = os.path.join(temp_dir, mmsi)\n",
    "        if os.path.isdir(mmsi_dir_path):\n",
    "            mmsi_tasks.append((mmsi, mmsi_dir_path, final_dir))\n",
    "    \n",
    "    # Process in parallel\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        results = [pool.imap_unordered(process_single_mmsi, mmsi_tasks)]\n",
    "        \n",
    "num_workers = cpu_count() - 1  # Leave 1 core free\n",
    "final_dir = 'data/final_processed'\n",
    "reduce(final_dir=final_dir, temp_dir=temp_dir, n_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ebe7c",
   "metadata": {},
   "source": [
    "### Combine vessel_types and cleanup temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "vessel_types_combined = dict()\n",
    "vessel_type_files = [f for f in os.listdir(vessel_type_dir) if f.startswith(\"vessel_types_\") and f.endswith(\".pkl\")]\n",
    "for vt_file in vessel_type_files:\n",
    "    vt_path = os.path.join(vessel_type_dir, vt_file)\n",
    "    with open(vt_path, \"rb\") as f:\n",
    "        vt_mapping = pickle.load(f)\n",
    "        vessel_types_combined.update(vt_mapping) # In case of conflicts, later files overwrite earlier ones\n",
    "    os.remove(vt_path)\n",
    "combined_vt_path = os.path.join(final_dir, \"vessel_types.pkl\")\n",
    "with open(combined_vt_path, \"wb\") as f:\n",
    "    pickle.dump(vessel_types_combined, f)\n",
    "    \n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190dcbc",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9eab8",
   "metadata": {},
   "source": [
    "## Something \"new\"\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915f629",
   "metadata": {},
   "source": [
    "## References\n",
    "D. Nguyen, R. Fablet. \"A Transformer Network With Sparse Augmented Data Representation and Cross Entropy Loss for AIS-Based Vessel Trajectory Prediction,\" in IEEE Access, vol. 12, pp. 21596–21609, 2024."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-tools-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
